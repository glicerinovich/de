{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Конвертируйте в HTML:\n",
    "\n",
    "Нажмите Ctrl + Shift + P (или Cmd + Shift + P на macOS), чтобы открыть палитру команд.\n",
    "\n",
    "Введите \"Jupyter: Export to HTML\" и выберите соответствующую опцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с файлами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Указание пути к папке, находящейся в рабочей директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Получаем текущую рабочую директорию\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Имя папки\n",
    "folder_name = \"папка\"\n",
    "\n",
    "# Создаем путь к целевой папке\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# Имя CSV файла\n",
    "file_name = \"ваш_файл.csv\"\n",
    "\n",
    "# Создаем полный путь к CSV файлу\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Читаем CSV файл\n",
    "with open(file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соединение файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соединение по порядку двух файлов\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка данных из CSV файлов\n",
    "df1 = pd.read_csv('file1.csv')\n",
    "df2 = pd.read_csv('file2.csv')\n",
    "\n",
    "# Вертикальное объединение по строкам\n",
    "concatenated_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Вывод результата\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соединение по ключевым столбцам двух файлов\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка данных из CSV файлов\n",
    "df1 = pd.read_csv('file1.csv')\n",
    "df2 = pd.read_csv('file2.csv')\n",
    "\n",
    "# Соединение по определенному столбцу (например, 'ключевой_столбец')\n",
    "merged_df = pd.merge(df1, df2, on='ключевой_столбец')\n",
    "\n",
    "# Соединение по нескольким столбцам\n",
    "merged_df = pd.merge(df1, df2, on=['ключевой_столбец1', 'ключевой_столбец2'])\n",
    "\n",
    "# Вывод результата\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соединение по порядку множества файлов в папке\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Путь к папке с файлами\n",
    "folder_path = '/путь/к/вашей/папке'\n",
    "\n",
    "# Список для хранения загруженных DataFrame\n",
    "dfs = []\n",
    "\n",
    "# Цикл по всем файлам в папке\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Убедитесь, что файлы имеют расширение .csv\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Объединение всех DataFrame из списка\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Вывод результата\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соединение по ключевым столбцам множества файлов в папке\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Путь к папке с файлами\n",
    "folder_path = '/путь/к/вашей/папке'\n",
    "\n",
    "# Столбец, по которому будет происходить объединение\n",
    "merge_column = 'ваш_столбец'\n",
    "\n",
    "# Инициализация DataFrame для хранения результатов\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Цикл по всем файлам в папке\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Объединение с существующим DataFrame\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on=merge_column)\n",
    "\n",
    "# Вывод результата\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исследование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Чтение данных из CSV файла\n",
    "df = pd.read_csv('ваш_файл.csv')\n",
    "\n",
    "# Вывод основной информации о DataFrame\n",
    "print(\"1. Общая информация о DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Вывод первых нескольких строк DataFrame\n",
    "print(\"\\n2. Первые несколько строк DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Вывод статистики для числовых признаков\n",
    "print(\"\\n3. Статистика для числовых признаков:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Вывод количества уникальных значений в каждом столбце\n",
    "print(\"\\n4. Количество уникальных значений в каждом столбце:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# Вывод количества пропущенных значений в каждом столбце\n",
    "print(\"\\n5. Количество пропущенных значений в каждом столбце:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Вывод уникальных значений для категориальных признаков\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "print(\"\\n6. Уникальные значения для категориальных признаков:\")\n",
    "for column in categorical_columns:\n",
    "    print(f\"{column}: {df[column].unique()}\")\n",
    "\n",
    "# Вывод корреляции между числовыми признаками\n",
    "print(\"\\n7. Корреляция между числовыми признаками:\")\n",
    "print(df.corr())\n",
    "\n",
    "# Вывод общей информации о наличии дублирующихся строк\n",
    "print(\"\\n8. Проверка наличия дублирующихся строк:\")\n",
    "print(f\"Есть дублирующиеся строки: {df.duplicated().any()}\")\n",
    "\n",
    "# Вывод общей информации о размере DataFrame (количество строк и столбцов)\n",
    "print(\"\\n9. Размер DataFrame (количество строк и столбцов):\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Чтение данных из CSV файла\n",
    "df = pd.read_csv('ваш_файл.csv')\n",
    "\n",
    "# Проверка и удаление дублирующихся записей\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Проверка и удаление пустых записей\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Поиск и обработка выбросов (используем метод межквартильного расстояния)\n",
    "def handle_outliers(column):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    df[column] = df[column].apply(lambda x: lower_bound if x < lower_bound else (upper_bound if x > upper_bound else x))\n",
    "\n",
    "# Метод межквартильного расстояния предпочтителен при обработке выбросов, поскольку он устойчив к асимметрии данных\n",
    "# и не подвержен влиянию экстремальных значений. Этот метод предоставляет робастное решение для выявления выбросов, минимизируя \n",
    "# воздействие крайних значений и делая его более надежным для анализа данных с неоднородным распределением.\n",
    "\n",
    "# Применение обработки выбросов к выбранным числовым столбцам (например, 'столбец1', 'столбец2')\n",
    "numeric_columns = ['столбец1', 'столбец2']\n",
    "for column in numeric_columns:\n",
    "    handle_outliers(column)\n",
    "\n",
    "# Поиск и обработка пропущенных значений\n",
    "# Для числовых признаков можно использовать заполнение средним значением\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Обработка пропущенных значений для числовых признаков\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for column in numeric_columns:\n",
    "    # Заполнение пропущенных значений средним\n",
    "    df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "# Обработка пропущенных значений для категориальных признаков\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    # Заполнение пропущенных значений модой\n",
    "    df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "\n",
    "# Обработка пропущенных значений для текстовых признаков\n",
    "text_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in text_columns:\n",
    "    # Заполнение пропущенных значений константой 'Unknown'\n",
    "    df[column].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Поиск и обработка всех категориальных признаков\n",
    "# Преобразование категориальных признаков с использованием one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['категориальный_столбец'])\n",
    "\n",
    "# Получение описательных статистик всех признаков из итогового набора полей\n",
    "descriptive_stats = df.describe()\n",
    "\n",
    "# Графики распределения всех признаков\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Графики числовых признаков\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()\n",
    "\n",
    "# Графики категориальных признаков\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(data=df, x=column)\n",
    "    plt.title(f'Count of {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другие действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение результата\n",
    "df.to_csv('новый_файл.csv', index=False)\n",
    "\n",
    "# Выборка по условию\n",
    "subset = df[df['столбец'] > 10]\n",
    "\n",
    "# Удаление столбцов\n",
    "df.drop(['столбец1', 'столбец2'], axis=1, inplace=True)\n",
    "\n",
    "# Обработка пропущенных значений\n",
    "df.dropna()  # Удаление строк с пропущенными значениями\n",
    "df.fillna(value)  # Замена пропущенных значений конкретным значением\n",
    "\n",
    "# Группировка данных\n",
    "grouped_data = df.groupby('столбец').mean()\n",
    "\n",
    "# Подсчет средних значений по столбцу 'столбец'\n",
    "grouped_data = df.groupby('столбец').mean()\n",
    "\n",
    "# Сортировка данных\n",
    "df.sort_values(by='столбец', ascending=False, inplace=True)\n",
    "\n",
    "# Преобразование столбца 'столбец' к типу данных int\n",
    "df['столбец'] = df['столбец'].astype(int)\n",
    "\n",
    "# Подсчет уникальных значений\n",
    "unique_values = df['столбец'].unique()\n",
    "\n",
    "# Вычисление основных статистических показателей\n",
    "mean_value = df['столбец'].mean() # Вычисление среднего значения столбца 'столбец'\n",
    "median_value = df['столбец'].median() # Вычисление медианы столбца 'столбец'\n",
    "std_dev = df['столбец'].std() # Вычисление стандартного отклонения столбца 'столбец'\n",
    "min_value = df['столбец'].min() # Нахождение минимального значения в столбце 'столбец'\n",
    "max_value = df['столбец'].max() # Нахождение максимального значения в столбце 'столбец'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Загрузка обработанного файла\n",
    "df = pd.read_csv('обработанный_файл.csv')\n",
    "\n",
    "# Предположим, что ваш целевой столбец называется 'целевой_столбец'\n",
    "X = df.drop('целевой_столбец', axis=1)\n",
    "y = df['целевой_столбец']\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Стандартизация данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 1. Логистическая регрессия\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = logistic_regression_model.predict(X_test_scaled)\n",
    "\n",
    "# Оценка модели логистической регрессии\n",
    "print(\"Логистическая регрессия:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# 2. Случайный лес\n",
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = random_forest_model.predict(X_test_scaled)\n",
    "\n",
    "# Оценка модели случайного леса\n",
    "print(\"\\nСлучайный лес:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# 3. Метод опорных векторов (SVM)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Оценка модели SVM\n",
    "print(\"\\nМетод опорных векторов (SVM):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Загрузка обработанного файла\n",
    "df = pd.read_csv('обработанный_файл.csv')\n",
    "\n",
    "# Выбор числовых признаков для кластеризации\n",
    "X = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Стандартизация данных\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 1. K-средних (KMeans)\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans_model.fit_predict(X_scaled)\n",
    "\n",
    "# Оценка модели KMeans с использованием silhouette_score\n",
    "silhouette_kmeans = silhouette_score(X_scaled, kmeans_labels)\n",
    "print(\"1. K-средних (KMeans) - Silhouette Score:\", silhouette_kmeans)\n",
    "\n",
    "# 2. Иерархическая кластеризация (AgglomerativeClustering)\n",
    "agg_model = AgglomerativeClustering(n_clusters=3)\n",
    "agg_labels = agg_model.fit_predict(X_scaled)\n",
    "\n",
    "# Оценка модели AgglomerativeClustering с использованием silhouette_score\n",
    "silhouette_agg = silhouette_score(X_scaled, agg_labels)\n",
    "print(\"2. Иерархическая кластеризация - Silhouette Score:\", silhouette_agg)\n",
    "\n",
    "# 3. DBSCAN\n",
    "dbscan_model = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan_model.fit_predict(X_scaled)\n",
    "\n",
    "# DBSCAN не имеет явного числа кластеров, поэтому silhouette_score может не применяться\n",
    "print(\"3. DBSCAN - No Silhouette Score for DBSCAN\")\n",
    "\n",
    "# Добавьте столбец с метками кластеров к исходному DataFrame\n",
    "df['KMeans_Cluster'] = kmeans_labels\n",
    "df['Agg_Cluster'] = agg_labels\n",
    "df['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "# Вывод полученных кластеров\n",
    "print(\"\\nКластеры, полученные с помощью KMeans:\")\n",
    "print(df['KMeans_Cluster'].value_counts())\n",
    "\n",
    "print(\"\\nКластеры, полученные с помощью Иерархической кластеризации:\")\n",
    "print(df['Agg_Cluster'].value_counts())\n",
    "\n",
    "print(\"\\nКластеры, полученные с помощью DBSCAN:\")\n",
    "print(df['DBSCAN_Cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Загрузка обработанного файла для задачи регрессии\n",
    "df_regression = pd.read_csv('обработанный_файл_регрессии.csv')\n",
    "\n",
    "# Предположим, что ваша целевая переменная называется 'целевая_переменная'\n",
    "X_reg = df_regression.drop('целевая_переменная', axis=1)\n",
    "y_reg = df_regression['целевая_переменная']\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Стандартизация данных\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_scaled_reg = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_scaled_reg = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# 1. Линейная регрессия\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_scaled_reg, y_train_reg)\n",
    "y_pred_linear_reg = linear_reg_model.predict(X_test_scaled_reg)\n",
    "\n",
    "# Оценка модели линейной регрессии\n",
    "print(\"1. Линейная регрессия:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_linear_reg))\n",
    "print(\"R-squared:\", r2_score(y_test_reg, y_pred_linear_reg))\n",
    "\n",
    "# 2. Случайный лес для регрессии\n",
    "random_forest_reg_model = RandomForestRegressor()\n",
    "random_forest_reg_model.fit(X_train_scaled_reg, y_train_reg)\n",
    "y_pred_rf_reg = random_forest_reg_model.predict(X_test_scaled_reg)\n",
    "\n",
    "# Оценка модели случайного леса для регрессии\n",
    "print(\"\\n2. Случайный лес для регрессии:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_rf_reg))\n",
    "print(\"R-squared:\", r2_score(y_test_reg, y_pred_rf_reg))\n",
    "\n",
    "# 3. Градиентный бустинг для регрессии\n",
    "gradient_boosting_reg_model = GradientBoostingRegressor()\n",
    "gradient_boosting_reg_model.fit(X_train_scaled_reg, y_train_reg)\n",
    "y_pred_gb_reg = gradient_boosting_reg_model.predict(X_test_scaled_reg)\n",
    "\n",
    "# Оценка модели градиентного бустинга для регрессии\n",
    "print(\"\\n3. Градиентный бустинг для регрессии:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_gb_reg))\n",
    "print(\"R-squared:\", r2_score(y_test_reg, y_pred_gb_reg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
